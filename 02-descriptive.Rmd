---
title: "Analysis of merged questionnaires"
author: "Anna Quaglieri & Riccardo Amorati"
date: "03/09/2017"
output:
  github_document:
    toc: yes
    toc_depth: 4
  html_document:
    toc: yes
    toc_depth: '4'
  html_notebook:
    code_folding: hide
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_float: yes
---

# Plan that I wrote with Richi's comments

Link at https://docs.google.com/document/d/1bdNeOMAYY90k8FAbPRWGBgS1YBEdP09kP0vcW0tNgPc/edit?usp=sharing


```{r,message=FALSE, echo=FALSE}
library(readr)
library(tidyverse)
library(reshape2)
library(corrplot)
library(psych)
library(pheatmap)
library(RColorBrewer)
library(cowplot)
library(gridExtra)
library(cowplot)
library(pheatmap)
library(sjPlot)
library(sjlabelled)
library(sjmisc)

data(efc)
theme_set(theme_sjplot())

# Chunk options
knitr::opts_chunk$set(echo = TRUE, prompt = TRUE,cache = TRUE,fig.width = 12,fig.height = 12)

```

# Read in data

```{r message=FALSE,message=FALSE}
european <- read_csv("01-cleaning_data_data/european_recoded.csv")
australian <- read_csv("01-cleaning_data_data/australian_recoded.csv")

dim(european)
dim(australian)

european$EU <- 1
australian$AU <- 1

all <- merge(european,australian,all = TRUE)

table(all$EU,all$AU,useNA = "always")
```

## Variables to describe dataset (can be different between contexts)

```{r}
demographics_var <- c("Age","Gender","L1","speak.other.L2","study.other.L2","origins","year.studyL2","other5.other.ways","degree","roleL2.degree","study.year","prof","L2.VCE","uni1.year","Context")
l2School <- "\\.L2school$"
l2School_variables <- colnames(all)[grep(l2School,colnames(all))]
```

- First language

```{r}
#table(all$L1,all$Context) # too many levels - needs to be cleaned (ex tot number of languages?)
table(all$L1,useNA = "always")
ggplot(all,aes(x=L1,fill=Context)) + geom_bar() + coord_flip() + ggtitle("First Language") + labs(y="N. of participants",x="")+theme_bw()
#table(all$speak.other.L2,all$Context)
```

```{r}
L2 <- data.frame(Freq=table(all$speak.other.L2)[order(table(all$speak.other.L2),decreasing = TRUE)],
                 L2=names(table(all$speak.other.L2))[order(table(all$speak.other.L2),decreasing = TRUE)]) # too many levels - needs to be cleaned (ex tot number of languages?)
head(L2)
```

- origins

```{r}
table(all$origins,useNA = "always")
```


```{r}
table(all$year.studyL2)
```

```{r}
table(all$degree)
```

- study.year in the European context is uni1.year in the Australian context

```{r}
all$study.year[is.na(all$study.year)] <- all$uni1.year[is.na(all$study.year)]
#table(all$study.year)
all$study.year <- ifelse(all$study.year == "Already graduated after 5 semesters in March 2016, was interested in survery/study, sorry.","6th semester",all$study.year)
table(all$study.year)
```

- proficiency

```{r}
table(all$prof,useNA = "always")
```

# Filter participants : keep only the ones that meet the inclusion criteria

```{r Filtered,fig.width=20,fig.height=15}
all$study.year[is.na(all$study.year)] <- all$uni1.year[is.na(all$study.year)]
# Filter only subject that we want to include in the study
# names(table(all$study.year))[1] = 1st semester"

filtered <- subset(all, (study.year == "1st year") | (study.year == names(table(all$study.year))[1]) & year.studyL2 != "0 years")

table(filtered$Context)
table(all$Context)
```

# Define demographic variables

## Need to check datasets with Rcihi (why do we have QC in L1? Am I using not the latest dataset?)

```{r message=FALSE}
all <- filtered
demographics_var <- c("Age","Gender","L1","speak.other.L2","study.other.L2","origins","year.studyL2","other5.other.ways","degree","roleL2.degree","study.year","prof","L2.VCE","uni1.year","Context")
l2School <- "\\.L2school$"
l2School_variables <- colnames(all)[grep(l2School,colnames(all))]

ggplot(all,aes(x=L1,fill=Context)) + geom_bar() + coord_flip() + ggtitle("First Language") + labs(y="N. of participants",x="") + theme_bw()

table(all$L1,all$Context)

table(all$degree,all$L1)
```

- Check for L1 but we decided not to filter for it

```{r}
#Filter by L1

nc <- names(table(all$Context))
table(all$Context)
l1_filter <- all[(all$Context == nc[1] & (all$L1 == "German" | all$L1 == "German and English")) | 
                    (all$Context == nc[2] & (all$L1 == "Italian")) | 
                    (all$Context == nc[3] & (all$L1 == "English" | all$L1 == "English and Dutch" | all$L1 == "German and English")) |
                    (all$Context == nc[4] & (all$L1 == "English" | all$L1 == "English and Dutch" | all$L1 == "German and English")),]


#all <- l1_filter

# do not filter for L1
all <- all

# subset demographics
demo <- subset(all,select=c("Resp.ID",demographics_var,l2School_variables))

# Numeri finali
table(l1_filter$Context)
table(all$Context)

```

- Filter missing value:
  - Filter participants who didn't put the degree
  - we don't care about speak.other.L2 and study.other.L2

```{r}
missing_bySample <- rowSums(is.na(demo))
names(missing_bySample) <- demo$Resp.ID
missing_byVar <- colSums(is.na(demo))
names(missing_byVar) <- colnames(demo)

barplot(missing_bySample)
d <- data.frame(miss=missing_byVar)
d$varID <- rownames(d)
ggplot(data=d,aes(x=varID,y=miss)) + geom_bar(stat="identity") + theme_bw() +theme(axis.text.x = element_text(angle = 45, hjust = 1)) 


demo_missing <- demo %>% group_by(Context) %>% summarise(roleL2.degree_na = sum(is.na(roleL2.degree)),
                                                         L2.VCE_na = sum(is.na(L2.VCE)),
                                                         other5.other.ways_na=sum(is.na(other5.other.ways )),
                                                         uni1.year_na = sum(is.na(uni1.year)),
                                                         primary1.L2school_na=sum(is.na(primary1.L2school)),
                                                         CLS3.L2school_na = sum(is.na(CLS3.L2school)),
                                                         VSL4.L2school_na=sum(is.na(VSL4.L2school)),
                                                         degree = sum(is.na(degree)),
                                                         schooL2country5.L2school_na=sum(is.na(schooL2country5.L2school)))

# We do not filter for speak.other.L2 or study.other.L2

#demo[is.na(demo$speak.other.L2),]
# teniamo
#demo[is.na(demo$study.other.L2),]
missing_bySample[names(missing_bySample) == "5166861581"]
#demo[is.na(demo$year.studyL2),]
missing_bySample[names(missing_bySample) == "5378798787"]

# remove NA from degree
#table(demo$degree,useNA = "always")
# Remove people
all <- all[!is.na(all$degree),]


table(all$Context)
```

## Write filtered and merged dataset

```{r}
write.csv(all,file.path("02-descriptive_data/context-merged_filtered.csv"))
```

## Descriptive plots and tables

- Summary demographics TO DO: - to change yearL2.study.richi 

```{r age_by_context,message=FALSE}
# add numbers on the bar

# tabAge <- t(table(all$Age,all$Context))
# ggplot(all,aes(x=Age,fill=Context)) + geom_bar(position="dodge",colour="white")   + labs(y="N participants") + scale_y_continuous(breaks=seq(0,90,10),limits=c(0,90)) + theme_bw() + draw_grob(tableGrob(tabAge), x=2.5, y=40, width=0.3, height=0.4) + ggtitle("Participants by age")
# tabAge

tabAge <- t(table(all$Age,all$Context))
ggdf <- data.frame(Age = rep(colnames(tabAge),each=4)[!(as.numeric(tabAge) == 0)],
  N.Participants = as.numeric(tabAge)[!(as.numeric(tabAge) == 0)],
  Context = rep(rownames(tabAge),times=3)[!(as.numeric(tabAge) == 0)])

ggplot(ggdf,aes(x=Age,y=N.Participants,fill=Context)) + geom_bar(position="dodge",colour="white",stat="identity")  + scale_y_continuous(breaks=seq(0,90,10),limits=c(0,90)) + theme_bw() + ggtitle("Participants by age")+
  geom_text(aes(label = N.Participants), hjust=0.5, vjust=-0.25, size = 2.5,position=position_dodge(width=0.9)) 

```

```{r gender_by_context,message=FALSE}
# add numbers on the bar

tabAge <- t(table(all$Gender,all$Context))
ggdf <- data.frame(Gender = rep(colnames(tabAge),each=4)[!(as.numeric(tabAge) == 0)],
  N.Participants = as.numeric(tabAge)[!(as.numeric(tabAge) == 0)],
  Context = rep(rownames(tabAge),times=3)[!(as.numeric(tabAge) == 0)])


ggplot(ggdf,aes(x=Gender,y=N.Participants,fill=Context)) + geom_bar(position="dodge",colour="white",stat="identity")  + labs(y="N participants") + scale_y_continuous(breaks=seq(0,90,10),limits=c(0,90)) + theme_bw() + ggtitle("Participants by gender")+  geom_text(aes(label = N.Participants), hjust=0.5, vjust=-0.25, size = 2.5,position=position_dodge(width=0.9)) 

```

```{r origns_by_context,message=FALSE}
# add numbers on the bar
tabAge <- t(table(all$origins,all$Context))
ggplot(all,aes(x=origins,fill=Context)) + geom_bar(position="dodge",colour="white") + ggtitle("Origins by context") + scale_y_continuous(breaks=seq(0,90,10),limits=c(0,90)) + theme_bw() + draw_grob(tableGrob(tabAge), x=2, y=60, width=0.3, height=0.4) + ggtitle("Participants by origins")
tabAge
```

- proficiency

```{r proficiency_by_context,message=FALSE}
tabAge <- t(table(all$prof,all$Context))
ggplot(all,aes(x=Context,fill=prof)) + geom_bar(position="dodge",colour="white") + ggtitle("Proficiency by context") + scale_y_continuous(breaks=seq(0,90,10),limits=c(0,90)) + theme_bw() + draw_grob(tableGrob(tabAge), x=2, y=80, width=0.3, height=0.4)
tabAge
```

- L2.VCE

```{r L2VCE_by_context,message=FALSE}
tabAge <- t(table(all[all$Context != "English in Germany" & all$Context != "English in Italy","L2.VCE"],all[all$Context != "English in Germany" & all$Context != "English in Italy",'Context'],useNA = "always"))
tabAge <- tabAge[-3,]

ggplot(all[all$Context != "English in Germany" & all$Context != "English in Italy",],aes(x=Context,fill=L2.VCE)) + geom_bar(position="dodge",colour="white") + ggtitle("L2.VCE by context") + scale_y_continuous(breaks=seq(0,90,10),limits=c(0,90)) + theme_bw() + draw_grob(tableGrob(tabAge), x=2, y=80, width=0.3, height=0.4)
```

- da mettere a posto con Richi

```{r year.studyL2_European_context,message=FALSE}
# year study L2
table(all$year.studyL2,all$other.year.studyL2.richi)

all$year.studyL2 <- ifelse(all$year.studyL2 == "Other",all$other.year.studyL2.richi,all$year.studyL2 )

# European context
ggplot(all[all$Context == "English in Germany" | all$Context == "English in Italy",],aes(x=degree,fill=year.studyL2)) + geom_bar(position="dodge",colour="white") + theme_bw() + ggtitle("Degree by study year L2, by Context") +  facet_grid(~Context,scales="free") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + labs(y = "N participants", x = "degree")
```

- Degree of enrolment

```{r degree_by_context,message=FALSE}
# Australian context
tabAge <- t(table(all[all$Context == "Italian in Australia" | all$Context == "German in Australia",'degree'],all[all$Context == "Italian in Australia" | all$Context == "German in Australia",'Context']))
ggplot(all[all$Context == "Italian in Australia" | all$Context == "German in Australia",],aes(x=Context,fill=degree)) + geom_bar(position="dodge",colour="white") + theme_bw() + ggtitle("Degree in Australian Contexts") + draw_grob(tableGrob(tabAge), x=1., y=40, width=0.3, height=0.4)
tabAge
```

```{r year.studyL2_Australian_context,message=FALSE}
# Australian context
tabAge <- t(table(all[all$Context == "English in Italy" | all$Context == "English in Germany",'degree'],all[all$Context == "English in Italy" | all$Context == "English in Germany",'Context']))

ggplot(all[all$Context == "English in Italy" | all$Context == "English in Germany",],aes(x=Context,fill=degree)) + geom_bar(position="dodge",colour="white") + theme_bw() + ggtitle("Degree in European Contexts")

tabAge
```

\clearpage

# Likert scales

```{r include=FALSE,message=FALSE}
likert_grep <- "\\.id$|\\.ought$|\\.intr$|\\.instru$|\\.integr$|\\.prof$|\\.post$|\\.comm$|^necessity$|^educated$"

# all
likert_variables_all <- colnames(all)[grep(likert_grep,colnames(all))]
likert_variables_all
likert_variables_all <- likert_variables_all[!(likert_variables_all %in% "other.prof")]
```


- scala di blues e strongly disagree in un colore completamente diverso

```{r fig.width=20,fig.height=20}
all_melt <- melt(all,id.vars = c("Resp.ID","Gender","Age","prof","Context","study.year"),
                        measure.vars = likert_variables_all)

all_melt$value <- factor(all_melt$value,levels=c("Strongly disagree","Disagree","Not sure","Agree","Strongly agree"))

all_melt <- all_melt %>% separate(variable,into=c("item","type"),sep="\\.",remove=FALSE)
ggplot(all_melt,aes(x=variable,fill=value)) + geom_bar(position = "stack",colour="black") + 
  facet_grid(Context~type,scales = "free")+theme(axis.text.x = element_text(angle = 45, hjust = 1),axis.text=element_text(size=8)) + ggtitle("Filtered dataset") + scale_fill_manual(values=c("#ca0020","#f4a582","#ffffbf","#abd9e9","#2c7bb6","grey"))

filt_sum <- all_melt %>% group_by(Context,variable,type,value) %>% dplyr::summarise(Ngroup=length(value))
ggplot(filt_sum,aes(x=value,y=Ngroup,colour=Context,group=interaction(variable, Context))) + geom_line() + geom_point() + facet_wrap(~type,scales = "free")+theme(axis.text.x = element_text(angle = 45, hjust = 1))

```


- **Convert Likert scales to numbers**


```{r fig.width=15,fig.height=15}

convertToNumber <- function(column){
  column <- factor(column,levels = c("Strongly disagree","Disagree","Not sure","Agree","Strongly agree"))
  column_number <- as.numeric(column)
  return(column_number)
}

table(all$Context)

convert_likert <- data.frame(apply(subset(all,select=likert_variables_all),2,convertToNumber))
colnames(convert_likert) <- paste0(colnames(convert_likert),"1")

likert_variables1 <- paste0(likert_variables_all,"1")

# join the converted variables to the filtered dataset
filtered_conv <- cbind(all,convert_likert)

table(filtered_conv[,likert_variables_all[4]],filtered_conv[,likert_variables1[4]],useNA = "always")

write.csv(filtered_conv,"02-descriptive_data/merged_filtered_likertNumber.csv",row.names = FALSE)
```

# Correlation plot of items by context

## Italian in Australia

```{r cor_italian_in_australia,fig.width=15,fig.height=15}
cov <- cor(filtered_conv[filtered_conv$Context == "Italian in Australia",likert_variables1[!(likert_variables1 %in% "necessity1")]],method = "pearson",use="pairwise.complete.obs")


row_infos <- data.frame(Variables=sapply(strsplit(colnames(cov),split="\\."),function(x) x[2]))
row_infos$Variables <- as.character(row_infos$Variables)
rownames(row_infos) <- rownames(cov)
row_infos$Variables[which(is.na(row_infos$Variables))] <- c("educated")
row_infos <- row_infos[order(row_infos$Variables),,drop=FALSE]

ann_col_wide <- data.frame(Variable=unique(row_infos$Variables))
ann_colors_wide <- list(Variables=c(comm1="#bd0026",educated="#b35806", id1="#f6e8c3",instru1="#35978f",integr1="#386cb0",intr1="#ffff99",ought1="grey",post1="black",prof1="pink"))

#pheatmap(cov, main = "Italian in Australia",annotation_names_row = FALSE,cluster_cols=TRUE,cluster_rows=TRUE,annotation_col = row_infos[,1,drop=FALSE], annotation_row = row_infos[,1,drop=FALSE],  annotation_colors = ann_colors_wide,breaks=seq(-1,1,0.2),col=c("#67001f","#b2182b","#d6604d","#f4a582","#fddbc7","#f7f7f7","#d1e5f0","#92c5de","#4393c3","#2166ac","#053061"),show_colnames = FALSE,width = 7,height = 7)
###################

diag(cov) <- NA
pheatmap(cov, main = "Italian in Australia",annotation_names_row = FALSE,cluster_cols=TRUE,cluster_rows=TRUE,annotation_col = row_infos[,1,drop=FALSE], annotation_row = row_infos[,1,drop=FALSE]
,  annotation_colors = ann_colors_wide,show_colnames = FALSE,breaks = seq(-0.6,0.7,length.out = 50),width = 7,height = 7,color=colorRampPalette(brewer.pal(n = 7, name = "RdBu"))(50))
```

## German in Australia

```{r cor_german_in_australia,fig.width=15,fig.height=15}
cov <- cor(filtered_conv[filtered_conv$Context == "German in Australia",likert_variables1[!(likert_variables1 %in% "necessity1")]],method = "pearson",use="pairwise.complete.obs")

row_infos <- data.frame(Variables=sapply(strsplit(colnames(cov),split="\\."),function(x) x[2]))
row_infos$Variables <- as.character(row_infos$Variables)
rownames(row_infos) <- rownames(cov)
row_infos$Variables[which(is.na(row_infos$Variables))] <- c("educated")
row_infos <- row_infos[order(row_infos$Variables),,drop=FALSE]

ann_col_wide <- data.frame(Variable=unique(row_infos$Variables))
ann_colors_wide <- list(Variables=c(comm1="#bd0026",educated="#b35806", id1="#f6e8c3",instru1="#35978f",integr1="#386cb0",intr1="#ffff99",ought1="grey",post1="black",prof1="pink"))

diag(cov) <- NA
pheatmap(cov, main = "German in Australia",annotation_names_row = FALSE,cluster_cols=TRUE,cluster_rows=TRUE,annotation_col = row_infos[,1,drop=FALSE], annotation_row = row_infos[,1,drop=FALSE]
,  annotation_colors = ann_colors_wide,show_colnames = FALSE,breaks = seq(-0.6,0.7,length.out = 50),width = 7,height = 7,color=colorRampPalette(brewer.pal(n = 7, name = "RdBu"))(50))
```

## English in Germany

```{r cor_english_in_germany,fig.width=15,fig.height=15}
cov <- cor(filtered_conv[filtered_conv$Context == "English in Germany",likert_variables1[!(likert_variables1 %in% c("reconnect.comm1",    "speakersmelb.comm1","comecloser.comm1","educated1"))]],method = "pearson",use="pairwise.complete.obs")

row_infos <- data.frame(Variables=sapply(strsplit(colnames(cov),split="\\."),function(x) x[2]))
row_infos$Variables <- as.character(row_infos$Variables)
rownames(row_infos) <- rownames(cov)
row_infos$Variables[which(is.na(row_infos$Variables))] <- c("necessity")
row_infos <- row_infos[order(row_infos$Variables),,drop=FALSE]

ann_col_wide <- data.frame(Variable=unique(row_infos$Variables))
ann_colors_wide <- list(Variables=c(id1="#f6e8c3",necessity="#b35806",instru1="#35978f",integr1="#386cb0",intr1="#ffff99",ought1="grey",post1="black",prof1="pink"))

diag(cov) <- NA
pheatmap(cov, main = "English in Germany",annotation_names_row = FALSE,cluster_cols=TRUE,cluster_rows=TRUE,annotation_col = row_infos[,1,drop=FALSE], annotation_row = row_infos[,1,drop=FALSE]
,  annotation_colors = ann_colors_wide,show_colnames = FALSE,breaks = seq(-0.6,0.7,length.out = 50),width = 7,height = 7,color=colorRampPalette(brewer.pal(n = 7, name = "RdBu"))(50))
```

## English in Italy

```{r cor_english_in_italy,fig.width=15,fig.height=15}
cov <- cor(filtered_conv[filtered_conv$Context == "English in Italy",likert_variables1[!(likert_variables1 %in% c("reconnect.comm1","speakersmelb.comm1","comecloser.comm1","educated1"))]],method = "pearson",use="pairwise.complete.obs")

row_infos <- data.frame(Variables=sapply(strsplit(colnames(cov),split="\\."),function(x) x[2]))
row_infos$Variables <- as.character(row_infos$Variables)
rownames(row_infos) <- rownames(cov)
row_infos$Variables[which(is.na(row_infos$Variables))] <- "necessity"
row_infos <- row_infos[order(row_infos$Variables),,drop=FALSE]

ann_col_wide <- data.frame(Variable=unique(row_infos$Variables))
ann_colors_wide <- list(Variables=c(comm1="#bd0026",necessity="#b35806", id1="#f6e8c3",instru1="#35978f",integr1="#386cb0",intr1="#ffff99",ought1="grey",post1="black",prof1="pink"))

diag(cov) <- NA
pheatmap(cov, main = "English in Italy",annotation_names_row = FALSE,cluster_cols=TRUE,cluster_rows=TRUE,annotation_col = row_infos[,1,drop=FALSE], annotation_row = row_infos[,1,drop=FALSE]
,  annotation_colors = ann_colors_wide,show_colnames = FALSE,breaks = seq(-0.6,0.7,length.out = 50),width = 7,height = 7,color=colorRampPalette(brewer.pal(n = 7, name = "RdBu"))(50))
```

## All context together

```{r cor_all_contexts}
cov <- cor(filtered_conv[,likert_variables1],method = "pearson",use="pairwise.complete.obs")

row_infos <- data.frame(Variables=sapply(strsplit(colnames(cov),split="\\."),function(x) x[2]))
row_infos$Variables <- as.character(row_infos$Variables)
rownames(row_infos) <- rownames(cov)
row_infos$Variables[which(is.na(row_infos$Variables))] <- c("necessity","educated")
row_infos <- row_infos[order(row_infos$Variables),,drop=FALSE]

ann_col_wide <- data.frame(Variable=unique(row_infos$Variables))
ann_colors_wide <- list(Variables=c(comm1="#bd0026",educated="orange", id1="#f6e8c3",instru1="#35978f",necessity="#b35806",integr1="#386cb0",intr1="#ffff99",ought1="grey",post1="black",prof1="pink"))

diag(cov) <- NA
pheatmap(cov, main = "All Contexts",annotation_names_row = FALSE,cluster_cols=TRUE,cluster_rows=TRUE,annotation_col = row_infos[,1,drop=FALSE], annotation_row = row_infos[,1,drop=FALSE]
,  annotation_colors = ann_colors_wide,show_colnames = FALSE,breaks = seq(-0.6,0.7,length.out = 50),width = 7,height = 7,color=colorRampPalette(brewer.pal(n = 7, name = "RdBu"))(50))

```

# Evaluate internal consistency of known constructs with alpha

```{r}
sets <- list(id.var=likert_variables1[grep("\\.id1$",likert_variables1)],
             ought.var=likert_variables1[grep("\\.ought1$",likert_variables1)],
             intr.var=likert_variables1[grep("\\.intr1$",likert_variables1)],
             instru.var=likert_variables1[grep("\\.instru1$",likert_variables1)],
             integr1.var=likert_variables1[grep("\\.integr1$",likert_variables1)],
             prof.var=likert_variables1[grep("\\.prof1$",likert_variables1)],
             post.var=likert_variables1[grep("\\.post1$",likert_variables1)],
             comm.var=likert_variables1[grep("\\.comm1$",likert_variables1)])
              

get_alpha <- function(dataMot,
                      var=sets$id.var){
  var_alpha <- alpha(dataMot[,var])
  dataf <- data.frame(alpha=var_alpha$total,
                    drop = var_alpha$alpha.drop)
  rownames(dataf) <- rownames(var_alpha$alpha.drop)
  return(dataf)
}

# "Italian in Australia"
ita_in_au <- do.call(rbind,lapply(sets,function(x) {
  get_alpha(data=filtered_conv[filtered_conv$Context == "Italian in Australia",],
                      var=x)}))
ita_in_au$var <- sapply(strsplit(rownames(ita_in_au),split="\\."),function(x) x[1]) 
ita_in_au$var.full <- sapply(strsplit(rownames(ita_in_au),split="\\."),function(x) x[3]) 
ita_in_au$Context <- "Italian in Australia"
rownames(ita_in_au) <- NULL

# "German in Australia"
germ_in_au <- do.call(rbind,lapply(sets,function(x) {
  get_alpha(data=filtered_conv[filtered_conv$Context == "German in Australia",],
                      var=x)}))
germ_in_au$var <- sapply(strsplit(rownames(germ_in_au),split="\\."),function(x) x[1]) 
germ_in_au$var.full <- sapply(strsplit(rownames(germ_in_au),split="\\."),function(x) x[3]) 
germ_in_au$Context <- "German in Australia"
rownames(germ_in_au) <- NULL

# "English in Germany"
eng_in_germ <- do.call(rbind,lapply(sets[!(names(sets) %in% "comm.var")],function(x) {
  get_alpha(data=filtered_conv[filtered_conv$Context == "English in Germany",],
                      var=x)}))

# the ones that makes issues
get_alpha(data=filtered_conv[filtered_conv$Context == "English in Germany",],
                      var=sets$ought.var)

eng_in_germ$var <- sapply(strsplit(rownames(eng_in_germ),split="\\."),function(x) x[1]) 
eng_in_germ$var.full <- sapply(strsplit(rownames(eng_in_germ),split="\\."),function(x) x[3]) 
eng_in_germ$Context <- "English in Germany"
rownames(eng_in_germ) <- NULL

# "English in Italy"
eng_in_ita <- do.call(rbind,lapply(sets[!(names(sets) %in% "comm.var")],function(x) {
  get_alpha(data=filtered_conv[filtered_conv$Context == "English in Italy",],
                      var=x)}))
eng_in_ita$var <- sapply(strsplit(rownames(eng_in_ita),split="\\."),function(x) x[1]) 
eng_in_ita$var.full <- sapply(strsplit(rownames(eng_in_ita),split="\\."),function(x) x[3]) 
eng_in_ita$Context <- "English in Italy"
rownames(eng_in_ita) <- NULL


# combine
full_alpha <- rbind(eng_in_ita,eng_in_germ,germ_in_au,ita_in_au)

```

- Plot alpha by variable

```{r alpha_chronbach_by_context}

full_alpha %>% group_by(Context,var) %>% 
  summarise(st.alpha = unique(alpha.std.alpha),
            G6=unique(alpha.G6.smc.)) %>%
  ggplot(.,aes(x=var,y=st.alpha,colour=Context)) + geom_point() + geom_line(aes(group=Context)) + theme_bw()

```


```{r fig.height=18,fig.width=14}
all_melt <- all_melt %>% separate(variable,into=c("item","type"),sep="\\.",remove=FALSE)
p1=ggplot(all_melt,aes(x=variable,fill=value)) + geom_bar(position = "stack") + 
  facet_grid(Context~type,scales = "free") + ggtitle("Filtered dataset")+theme(axis.text.x = element_text(angle = 45, hjust = 1),axis.text=element_text(size=8))+theme_bw()

p2=ggplot(full_alpha,aes(x=var.full,y=drop.std.alpha,colour=Context)) + geom_point() + geom_line(aes(group=Context)) + theme_bw() + facet_wrap(~var,scales="free")

p4=ggplot(full_alpha,aes(x=var.full,y=drop.average_r,colour=Context)) + geom_point() + geom_line(aes(group=Context)) + theme_bw() + facet_wrap(~var,scales="free")

p3=full_alpha %>% group_by(Context,var) %>% 
  summarise(st.alpha = unique(alpha.std.alpha),
            G6=unique(alpha.G6.smc.)) %>%
  ggplot(.,aes(x=var,y=st.alpha,colour=Context)) + geom_point() + geom_line(aes(group=Context)) + theme(axis.text.x = element_text(angle = 45, hjust = 1),axis.text=element_text(size=8)) + theme_bw()


cowplot::plot_grid(p2,p3,nrow=2)

```

\clearpage

# Factor Analysis with Robby's function

## Read in data

```{r}
all <- read.csv(file.path("02-descriptive_data/merged_filtered_likertNumber.csv"))
```

## Likert variables

```{r include=FALSE,message=FALSE}
likert_grep <- "\\.id$|\\.ought$|\\.intr$|\\.instru$|\\.integr$|\\.prof$|\\.post$|\\.comm$|^necessity$|^educated$"

# all
likert_variables_all <- colnames(all)[grep(likert_grep,colnames(all))]
likert_variables_all
likert_variables_all <- likert_variables_all[!(likert_variables_all %in% "other.prof")]

# likert variables converted to numbers
likert_variables1 <- paste0(likert_variables_all,"1")

```

## Alpha and FA with the combined dataset

```{r}
dat <- all[,likert_variables1[!(likert_variables1 %in% c("necessity1","educated1"))]]
psych::alpha(dat,use="pairwise.complete.obs")

#detach("package:ggplot2", unload=TRUE)


fa_all <- function(data4,rot,minL,maxL,nfac=0,seed=5){
  
  set.seed(seed)
  
  # Save the orignal dataset
  data_orig <- data4
  
  # Define the rotations
  orth <- c("varimax", "quartimax", "bentlerT", "equamax", "varimin", "geominT" , "bifactor" )
  obl <- c("Promax", "promax", "oblimin", "simplimax", "bentlerQ", "geominQ", "biquartimin" ,"cluster")
  
  
  
  alpha2 <- function(x){
    
    alpha.1 <- function(C, R) {
      n <- dim(C)[2]
      alpha.raw <- (1 - tr(C)/sum(C)) * (n/(n - 1))
      sumR <- sum(R)
      alpha.std <- (1 - n/sumR) * (n/(n - 1))
      smc.R <- smc(R)
      G6 <- (1 - (n - sum(smc.R))/sumR)
      av.r <- (sumR - n)/(n * (n - 1))
      mod1 <- matrix(av.r, n, n)
      Res1 <- R - mod1
      GF1 = 1 - sum(Res1^2)/sum(R^2)
      Rd <- R - diag(R)
      diag(Res1) <- 0
      GF1.off <- 1 - sum(Res1^2)/sum(Rd^2)
      sn <- n * av.r/(1 - av.r)
      Q = (2 * n^2/((n - 1)^2 * (sum(C)^3))) * (sum(C) * (tr(C %*% 
                                                               C) + (tr(C))^2) - 2 * (tr(C) * sum(C %*% C)))
      result <- list(raw = alpha.raw, std = alpha.std, G6 = G6, 
                     av.r = av.r, sn = sn, Q = Q, GF1, GF1.off)
      return(result)
    }
    
    
    if (!isCorrelation(x)) {
      item.var <- apply(x, 2, sd, na.rm = T)
      bad <- which((item.var <= 0) | is.na(item.var))
      if ((length(bad) > 0) && delete) {
        for (baddy in 1:length(bad)) {
          warning("Item = ", colnames(x)[bad][baddy], " had no variance and was deleted")
        }
        x <- x[, -bad]
        nvar <- nvar - length(bad)
      }
      response.freq <- response.frequencies(x, max = 10)
      C <- cov(x, use = "pairwise")
    }
    else {
      C <- x
    }
    
    
    
    R <- cov2cor(C)
    
    alpha.total <- alpha.1(C, R)
    
    return(alpha.total)
  }
  
  
  cicl<-0
  par1<-1
  par2<-1
  par3<-1
  while(par3>0){
    
    
    # Selezione il numero di fattori consigliati
    
    cat("Calculating the number of factors needed \n")
    
    fact <- nfac
    if(nfac==0){
      fact<-fa.parallel(data4)$nfact
    }
    
    if(fact==1){
      stop("Only one factor remains. Check the data or reduce the threshold")
    }
    
    #rot<-c("none", "varimax", "quartimax", "bentlerT", "geominT" , "bifactor", "promax", "oblimin", "simplimax", "bentlerQ", "geominQ" , "biquartimin" , "cluster" )
    
    # Matrice uota che conterra le miedie degli alpha
    
    range <- length(seq(minL,maxL,0.01))
    
    
    mean.al<-matrix(0,range,length(rot))
    
    # Sottociclo per il calcolo delle medie degli alpha (sui fattori) al variare sia della soglia sia della della rotazione
    
    cat("Calculating the loadings thresholds \n")
    
    for(b in 1:length(rot)){
      cat("- Calculating the loadings thresholds for rotation",rot[b],"\n")
      fa1<-fa(data4,fact,rotate=rot[b])
      
      a<-fa1$loadings
      class(a)<-"matrix"
      colnames(a)<-paste("F",1:fact,sep="")
      a<-as.data.frame(a)
      a<-round(a,2)
      a$D<-rownames(a)
      
      ls1<-minL
      m<-rep(0,range)
      i<-1
      nv<-fact
      
      while(ls1<=maxL+0.01){
        
        var<-lapply(1:nv,function(f)unique(a$D[abs(a[,f])>ls1]))
        names(var)<-paste(colnames(a[,1:nv]))
        
        
        # Do this if there are factors with length less than 2
        if(any(as.numeric(summary(var)[,1])<2)){
          
          # If a certain threshold and rotation gives only factors composed by 1 give it alpha=0
          if(sum(as.numeric(summary(var)[,1])>1)==0){
            m[i] <- 0
          }else{
            var1<-var[as.numeric(summary(var)[,1])>1]
            al<-sapply(1:length(var1),function(v)alpha2(data4[,var1[[v]]])$std)
            al<-data.frame(al)
            #m[i]<-mean(t(al))
            m[i]<-median(t(al))
          }
          
        }
        # Do this if ALL the factors have length greater than 1
        else{
          al<-sapply(1:nv,function(v)alpha2(data4[,var[[v]]])$std)
          al<-data.frame(al)
          #m[i]<-mean(t(al))
          m[i]<-median(t(al))
        }
        
        i<-i+1
        ls1<-ls1+0.01
        #cat(ls1)
      }
      mean.al[,b]<-m
      #cat("\n")
    }
    
    # Creazione e stampa degli andamenti delle medie degli alpha
    cat("Producing the threshold plot \n")
    
    mean.al<-as.data.frame(mean.al)
    colnames(mean.al)<-rot
    mean.al$sl<-seq(minL,maxL,0.01)
    mean.al1<-melt(mean.al,id.vars="sl")
    zp<-ggplot(mean.al1,aes(x=sl,y=value,colour=variable))+geom_line()+labs(x="Soglia Loading",y="Alpha Medio",colour="Rotazione")
    
    
    # Display the plot
    print(zp)
    
    max(mean.al)
    #print(mean.al)
    
    # Selezione della rotazione e della soglia che massimizzano le medie degli alpha
    cat("Choosing the best rotation and threshold \n")
    ind<-which(mean.al==max(mean.al),arr.ind=T)
    if(class(ind)=="matrix"){
      ind<-ind[1,]
    }
    
    rota<-names(mean.al)[ind[2]]
    sogl<-mean.al$sl[ind[1]]
    
    
    # Keep the same rotation that was selected in the first run
    rot <- rota
    
    
    # Calcolo fattoriale
    
    fa1<-fa(data4,fact,rotate=rota)
    a<-fa1$loadings
    class(a)<-"matrix"
    colnames(a)<-paste("F",1:fact,sep="")
    a<-as.data.frame(a)
    a<-round(a,2)
    a$D<-rownames(a)
    
    # Creazione dei fattori
    
    var<-lapply(1:fact,function(f)unique(a$D[abs(a[,f])>sogl]))
    names(var)<-paste(colnames(a[,1:fact]))
    
    # Display the factors
    cat("Displaying the factors \n")
    print(var)
    
    # togliamo le variabili che non entrano nei fattori al secondo ciclo
    
    nc<- ncol(data4)
    par1_names <- names(data4)[!(names(data4) %in% unlist(var))]
    data4<-data4[,names(data4) %in% unlist(var)]
    
    # Aggiorniamo il parametro di ciclo
    
    par1<-nc-ncol(data4) 
    
    # togliamo le variabili che entrano in un fattore da sole (solo se non entrano in un altro fattore)
    par4 <- 0
    par4_names <- character()
    
    len_fac <- cbind(1:fact,sapply(1:fact,function(v)length(var[[v]])))
    
    if(any(len_fac[,2]==1)){
      par4_nam<- data.frame(variable=as.character(unlist(var[ c(len_fac[len_fac[,2]==1,1])  ])))
      par4_nam$variable <- as.character(par4_nam$variable)
      par4_nam$ntimes= sapply(1:nrow(par4_nam), function(v)sum(unlist(var)%in%par4_nam[v,"variable"] ) )
      # Do this only if the single variable enetrs in a variable alone
      
      if(any(par4_nam$ntimes>1) & rot%in%obl ){
        cat("- Will NOT remove variable(s)", par4_nam$variable[par4_nam$ntimes>1],"since they contribute to multiple factors","\n")
      }
      
      par4_names <- par4_nam$variable[par4_nam$ntimes==1]
      par4 <- length(par4_names)
      data4<-data4[,!names(data4) %in% par4_names]
    }
    
    
    # Togliamo le variabili che compaiono in piu fattori (se la rotazione e obliqua non effetuiamo tale operazione)
    
    if(rota%in%obl){
      par2<-0
      s <- character()
    }else{
      s<-c(as.vector(unlist(var)))
      s<-unique(s[duplicated(s)])
      par2<-length(s)
      data4<-data4[,!names(data4) %in% s]
    }
    
    
    
    
    cicl<-cicl+1
    
    # Aggiorniamo il parametro di ciclo
    
    par3<-par1+par2+par4
    
    # Stampa diagnosi ciclo
    
    cat(paste("Unused Variable:",par1),"\n")
    cat("-",paste(par1_names),"\n")
    cat(paste("Repeated variables:",par2),"\n")
    cat("-",paste(s),"\n")
    cat(paste("Single variables:",par4),"\n")
    cat("-",paste(par4_names),"\n")
    cat(paste("Rotation used:",rota),"\n")
    cat(paste("Threshold chosen:",sogl),"\n")
    cat(paste("End interation",cicl),"\n","\n")
    
  }
  
  cat(paste("Variables excluded in the process:"),names(data_orig)[!names(data_orig)%in%names(data4)],"\n")
  return(data4)
}



#

likert_variables2 <- names(dat)

data1 <- dat[,likert_variables1[!(likert_variables1 %in% c("necessity1","educated1","reconnect.comm1", "speakersmelb.comm1", "comecloser.comm1"))]]

# Plot the correlations
corrplot(cor(data1,use = "pair"))


# check which variable does not correlated with any other vaiable
r1 <- cor(data1,use = "pair")
diag(r1) <- 0
r1 <- data.frame(r1)
temp <- data.frame(name=names(r1),cor=sapply(1:ncol(r1),function(v)any(r1[,v]>=0.3))  )
as.character(temp$name[temp$cor==F])




# Keep just the itemes with a r.cor greater or equal to 0.3
as <- psych::alpha(data1,check.keys=F)$item.stats
as$n1<-1:nrow(as)
summary(as$r.cor)
no_corr_macu <- rownames(as[abs(as$r.cor)<0.3,])
no_corr_macu
data1<-data1[,!names(data1)%in%no_corr_macu]




# check which items will decrease the alpha
al<- psych::alpha(data1)
drop<-al$alpha.drop
tot<-as.numeric(al$total$std.alpha)
drop <- drop[drop$std.alpha>tot,]
drop


# Check how much
drop$std.alpha-tot
# They don;t drop enough so leave it

#drop_alpha_macu <- rownames(drop)
#data1<-data1[,!names(data1)%in%drop_alpha_macu]




# check how many factors should be used
fap <- fa.parallel(data1)
fap


data_macu <- data1


## Perform the anlaysis for macular thickness



#Check again the out_maculiers
out_macu <- outlier(data_macu)
#abline(h=800)

#dat_imp_pheno[out_macu>800,1:20]
#out_maculiers_macu <- dat_imp_pheno$id1[out_macu>800]
#data_macu <- data_macu[out_macu<800,]




# Run the first set of analysisi and check what is cleaned out_macu!
rot=c("oblimin","promax")


# Take off the variables that give problem
#problem_var_macu <- c( "v07_spectralis" ,"v01_cyrrus" )
#data_macu <- data_macu[,!names(data_macu)%in%problem_var_macu]


library(ggplot2)
data_macu_facleaned <- fa_all(data_macu,rot,0.2,0.5)

# 0 Variable do not enter the factors
not_used_fact_macu <- names(data_macu)[!names(data_macu)%in%names(data_macu_facleaned)]
not_used_fact_macu

# Check again how many factors you need
#fa.parallel(data_macu_facleaned)

nfact_macu <- 6

# Run the actual factorial analysis on the final dataset
fa_macu<-fa(data_macu_facleaned,nfact_macu,rot="promax")

# Check whther any variable do not enter in any factor
lod <- fa_macu$loadings
class(lod) <- "matrix"
lod <- data.frame(lod)
lod$any <- apply(lod,1,function(v)any(abs(v)>=0.2) )
apply(lod[,1:nfact_macu],1,max )
lod[lod$any==F,]
# NONE, good!

# Plot the results
a<-fa_macu$loadings
class(a)<-"matrix"
colnames(a)<-paste("F",1:nfact_macu,sep="")
a<-as.data.frame(a)
a<-round(a,2)
a$D<-rownames(a)
a1 <- a
a1$D
a1 <- melt(a1,id.vars=c("D"))
a1$x <- runif(nrow(a1))
a1$inv <- ifelse(a1$value<0,"neg","pos")
a1$value[abs(a1$value)<0.2] <- 0
a1 <- a1[a1$value!=0,]

ggplot(a1)+geom_bar(aes(x=reorder(D, value) ,y=value),stat="identity")+facet_wrap(~variable,ncol = 2,scales = "free_y")+coord_flip()
#detach("package:ggplot2", unload=TRUE)


var<-lapply(1:nfact_macu,function(f)unique(a$D[abs(a[,f])>0.2]))
names(var)<-paste(colnames(a[,1:nfact_macu]))

al <- sapply(1:length(var),function(v) psych::alpha(data_macu_facleaned[,var[[v]]])$total$std.alpha)
al
# Alpha total
psych::alpha(data_macu_facleaned)$total$std.alpha
# they are ok...
psych::alpha(data_macu)$total$std.alpha


# Table of the factors
a$D <- NULL
a[abs(a)<0.2] <- 0
for(i in 1:ncol(a)){a[,i] <- as.character(a[,i])}

a[a=="0"] <- ""
loading_fact_macu <- a
loading_fact_macu

# Discriminant analysis: In this section i will test if the values of the variables kept in the dataframe by the factorial analysis are able to discriminate between subjects for which their total score lie in the 1st and 3rd quartile.
discr<-unique(data_macu_facleaned)

# Detrmine the total score

discr$punteggio<-rowSums(discr)

# Dividce the groups of people who lie in the 4rth and 1st quarile

hist(discr$punteggio)
quantile(discr$punteggio,na.rm = T)

discr1<-unique(discr[discr[,ncol(discr)]<=quantile(discr[,ncol(discr)],na.rm = T)[2],])
discr2<-unique(discr[discr[,ncol(discr)]>=quantile(discr[,ncol(discr)],na.rm = T)[4],])

# Wilcox Test the values of each single variable comparing the group of pople lien in the 1st and 3rd quartile

test<-data.frame(Item=colnames(discr[,1:(ncol(discr)-1)]),p.value=rep(0,(ncol(discr)-1)))

for(i in 1:(ncol(discr)-1)){
  test[i,2]<-wilcox.test(discr1[,i],discr2[,i],alternative="two.sided")$p.value
}

test <- test[order(test$p.value),]
test
# they all discriminate!


# Calculate factors on the discarded variables

dat_disc <- dat[,no_corr_macu]


# check how many factors should be used
fap <- fa.parallel(dat_disc)
fap



library(ggplot2)
fa_disc <- fa(dat_disc,nfactors = 1,rotate = "oblimin")



# Plot the results
a<-fa_disc$loadings
class(a)<-"matrix"
colnames(a)<-paste("F",1,sep="")
a<-as.data.frame(a)
a<-round(a,2)
a$D<-rownames(a)
a1 <- a
a1$D
a1 <- melt(a1,id.vars=c("D"))
a1$x <- runif(nrow(a1))
a1$inv <- ifelse(a1$value<0,"neg","pos")
a1$value[abs(a1$value)<0.2] <- 0
a1 <- a1[a1$value!=0,]



library(ggplot2)
ggplot(a1)+geom_bar(aes(x=reorder(D, value) ,y=value),stat="identity")+facet_wrap(~variable,ncol = 2,scales = "free_y")+coord_flip()
#detach("package:ggplot2", unload=TRUE)


# Predict the factors

pred <- as.data.frame(predict(fa_macu,dat[,names(data_macu_facleaned)]))
names(pred) <- paste("Factor",1:nfact_macu,sep = "")

# Predict the factor from the discarded variables
pred_disc <- as.data.frame(predict(fa_disc,dat[,names(dat_disc)]))
names(pred_disc) <- paste("Factor",7,sep = "")



factors <- c(names(pred),names(pred_disc))


dat_complete <- cbind(dat,scale(pred),scale(pred_disc))


corrplot(cor(dat_complete[,likert_variables2],dat_complete[,factors],use = "pair"))


all_complete <-  cbind(all,pred,pred_disc)

dat_plot <- melt(all_complete,id.vars = "Context",measure.vars = factors)

library(ggplot2)
ggplot(dat_plot)+geom_boxplot(aes(x=Context,y=value,color=Context))+facet_wrap(~variable)+coord_flip()+guides(color=F)



mod <- lm(Factor1~Context,data=all_complete)
summary(mod)

mod <- lm(Factor2~Context,data=all_complete)
summary(mod)

summary(lm(Factor4~Context,data=all_complete))

summary(lm(Factor6~Context,data=all_complete))

summary(lm(Factor7~Context,data=all_complete))

```

## Basic factor analysis: 7 factors

Seven, is the number of factors that would be present according to the study design.
Using very relaxed cutoff of 0.2 to get rid of not important variables in each factor.

```{r}
usable_items <- likert_variables1[!(likert_variables1 %in% c("necessity1","educated1","reconnect.comm1", "speakersmelb.comm1", "comecloser.comm1"))]
data1 <- dat[,usable_items]

fact <- 7
fa_basic <- fa(data1,fact)

fa_basic

# plot loadings
loadings_basic <- fa_basic$loadings
class(loadings_basic)<-"matrix"
colnames(loadings_basic)<-paste("F",1:7,sep="")
loadings_basic<-as.data.frame(loadings_basic)
loadings_basic<-round(loadings_basic,2)
loadings_basic$D<-rownames(loadings_basic)
a1 <- loadings_basic

a1 <- melt(a1,id.vars=c("D"))
a1$x <- runif(nrow(a1))
a1$inv <- ifelse(a1$value<0,"neg","pos")
a1$value[abs(a1$value)<0.2] <- 0
a1 <- a1[a1$value!=0,]
a1 <- a1 %>% separate(D,into = c("Variable","Item"),remove=FALSE,sep="[.]")

ggplot(a1)+geom_bar(aes(x=reorder(D, value) ,y=value,fill=Item),stat="identity")+facet_wrap(~variable,ncol = 2,scales = "free_y")+coord_flip() + geom_hline(yintercept = c(-0.3,0.3),linetype="dotted",colour="dark red")

# Table of the factors
loadings_basic$D <- NULL
loadings_basic[abs(loadings_basic)<0.2] <- 0
for(i in 1:ncol(loadings_basic)){loadings_basic[,i] <- as.character(loadings_basic[,i])}

loadings_basic[loadings_basic=="0"] <- ""
loading_fact_reduced <- loadings_basic
loading_fact_reduced

# predict values per samples
pred_basic <- as.data.frame(predict(fa_basic,data1))
names(pred_basic) <- paste("Factor",1:fact,sep = "")

factors <- names(pred_basic)
dat_complete_basic <- cbind(dat,scale(pred_basic))
corrplot(cor(dat_complete_basic[,usable_items],dat_complete_basic[,factors],use = "pair"))

all_complete_basic <-  cbind(all,pred_basic)
dat_plot_basic <- melt(all_complete_basic,id.vars = "Context",measure.vars = factors)

library(ggplot2)
ggplot(dat_plot_basic)+geom_boxplot(aes(x=Context,y=value,color=Context))+facet_wrap(~variable)+coord_flip()+guides(color=F)


```


## Basic factor analysis: 6 factors

Using very relaxed cutoff of 0.2 to get rid of not important variables in each factor.

```{r}
usable_items <- likert_variables1[!(likert_variables1 %in% c("necessity1","educated1","reconnect.comm1", "speakersmelb.comm1", "comecloser.comm1"))]
data1 <- dat[,usable_items]

# From a statisticak point of view 
fap <- fa.parallel(data1)
fact <- 6
fa_basic <- fa(data1,fact)

fa_basic

# plot loadings
loadings_basic <- fa_basic$loadings
class(loadings_basic)<-"matrix"
colnames(loadings_basic)<-paste("F",1:6,sep="")
loadings_basic<-as.data.frame(loadings_basic)
loadings_basic<-round(loadings_basic,2)
loadings_basic$D<-rownames(loadings_basic)
a1 <- loadings_basic

a1 <- melt(a1,id.vars=c("D"))
a1$x <- runif(nrow(a1))
a1$inv <- ifelse(a1$value<0,"neg","pos")
a1$value[abs(a1$value)<0.2] <- 0
a1 <- a1[a1$value!=0,]
a1 <- a1 %>% separate(D,into = c("Variable","Item"),remove=FALSE,sep="[.]")

ggplot(a1)+geom_bar(aes(x=reorder(D, value) ,y=value,fill=Item),stat="identity")+facet_wrap(~variable,ncol = 2,scales = "free_y")+coord_flip()+ geom_hline(yintercept = c(-0.3,0.3),linetype="dotted",colour="dark red")

# Table of the factors
loadings_basic$D <- NULL
loadings_basic[abs(loadings_basic)<0.2] <- 0
for(i in 1:ncol(loadings_basic)){loadings_basic[,i] <- as.character(loadings_basic[,i])}

loadings_basic[loadings_basic=="0"] <- ""
loading_fact_reduced <- loadings_basic
loading_fact_reduced

# predict values per samples
pred_basic <- as.data.frame(predict(fa_basic,data1))
names(pred_basic) <- paste("Factor",1:fact,sep = "")

factors <- names(pred_basic)
dat_complete_basic <- cbind(dat,scale(pred_basic))
corrplot(cor(dat_complete_basic[,usable_items],dat_complete_basic[,factors],use = "pair"))

all_complete_basic <-  cbind(all,pred_basic)
dat_plot_basic <- melt(all_complete_basic,id.vars = "Context",measure.vars = factors)

library(ggplot2)
ggplot(dat_plot_basic)+geom_boxplot(aes(x=Context,y=value,color=Context))+facet_wrap(~variable)+coord_flip()+guides(color=F)


```


